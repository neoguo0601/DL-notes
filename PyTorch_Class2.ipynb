{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 可以利用torch.nn包来创建神经网络, nn依靠autograd来定义模型并且对其计算微分. 从nn.Module类派生的子类中会包含模型的layers, 子类的成员函数forward(input)会返回模型的运行结果.\n",
    "\n",
    "##### 经典的训练神经网络的过程包含以下步骤:\n",
    "\n",
    "定义具有一些可学习参数(权重)的神经网络\n",
    "\n",
    "在数据集上创建迭代器\n",
    "\n",
    "将数据送入到网络中处理\n",
    "\n",
    "计算loss\n",
    "\n",
    "对参数进行反向求导\n",
    "\n",
    "更新参数: weight=weight−lr∗gradient weight = weight - \n",
    "lr*gradientweight=weight−lr∗gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 当定义好模型的forward()函数以后, backward()函数就会自动利用autograd机制定义, 无需认为定义.\n",
    "\n",
    "\n",
    "\n",
    "### 可以通过net.parameters()函数来获取模型中可学习的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = net.parameters()\n",
    "params = list(params)\n",
    "print(len(params))\n",
    "#print(params.size())\n",
    "print(params[0].size())\n",
    "print(params[1].size())\n",
    "print(params[2].size())\n",
    "print(params[3].size())\n",
    "print(params[4].size())\n",
    "print(params[5].size())\n",
    "print(params[6].size())\n",
    "print(params[7].size())\n",
    "print(params[8].size())\n",
    "print(params[9].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据网络结构接受的输入, 想网络中传输数据并获取计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1126,  0.0531, -0.0047, -0.0902, -0.0512, -0.0102, -0.1489, -0.0216,\n",
      "          0.0790,  0.1087]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的代码可以清空梯度缓存并计算所有需要求导的参数的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需要注意的是, 整个torch.nn包只支持mini-batches, 所以对于单个样本, 也需要显示指明batch size=1, 即input第一个维度的值为1\n",
    "\n",
    "### 也可以对单个样本使用input.unsqueeze(0)来添加一个假的batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个损失函数往往接收的是一对儿数据 (output, target). 然后根据相应规则计算output和target之间相差多远, 如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5499, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1,-1) # 令target和output的shape相同.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss) # tensor(1.3638, grad_fn=<MseLossBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用.grad_fn属性, 可以看到关于loss的计算图:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000002BFC56D0C50>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) # 返回MseLossBackward对象\n",
    "#input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "#      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "#      -> MSELoss\n",
    "#      -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此, 当调用loss.backward()时, 就会计算出所有(requires_grad=True的)参数关于loss的梯度, 并且这些参数都将具有.grad属性来获得计算好的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再利用loss.backward()计算梯度之前, 需要先清空已经存在的梯度缓存(因为PyTorch是基于动态图的, 每迭代一次就会留下计算缓存, 到一下次循环时需要手动清楚缓存), 如果不清除的话, 梯度就换累加(注意不是覆盖)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 0.0039, -0.0066,  0.0189, -0.0069, -0.0043,  0.0155])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()  # 清楚缓存\n",
    "print(net.conv1.bias.grad) # tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(net.conv1.bias.grad) # tensor([ 0.0181, -0.0048, -0.0229, -0.0138, -0.0088, -0.0107])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update The Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的更新方法是按照权重的更新公式:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(learning_rate*f.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当希望使用一些不同的更新方法如SGD, Adam等时, 可以利用torch.optim包来实现, 如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01) # 创建优化器\n",
    "optimizer.zero_grad() # 清空缓存\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward() # 计算梯度\n",
    "optimizer.step() # 执行一次更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, 1, 0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=1000): # <======== modificaition to comply fast.ai\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            ConvBlock(in_channels=3, out_channels=32),\n",
    "            ConvBlock(in_channels=32, out_channels=64),\n",
    "            ConvBlock(in_channels=64, out_channels=128),\n",
    "            ConvBlock(in_channels=128, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=512),\n",
    "            ConvBlock1(in_channels=512, out_channels=1024),\n",
    "            ConvBlock1(in_channels=1024, out_channels=1024),\n",
    "            ConvBlock(in_channels=1024, out_channels=512)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # <======== modificaition to comply fast.ai\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512*2*2, 1024),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #x = self.avgpool(x)\n",
    "        #x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             896\n",
      "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
      "              ReLU-3         [-1, 32, 128, 128]               0\n",
      "         ConvBlock-4           [-1, 32, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]          18,496\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "         ConvBlock-8           [-1, 64, 32, 32]               0\n",
      "            Conv2d-9          [-1, 128, 32, 32]          73,856\n",
      "      BatchNorm2d-10          [-1, 128, 32, 32]             256\n",
      "             ReLU-11          [-1, 128, 32, 32]               0\n",
      "        ConvBlock-12          [-1, 128, 16, 16]               0\n",
      "           Conv2d-13          [-1, 256, 16, 16]         295,168\n",
      "      BatchNorm2d-14          [-1, 256, 16, 16]             512\n",
      "             ReLU-15          [-1, 256, 16, 16]               0\n",
      "        ConvBlock-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17            [-1, 512, 8, 8]       1,180,160\n",
      "      BatchNorm2d-18            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-19            [-1, 512, 8, 8]               0\n",
      "        ConvBlock-20            [-1, 512, 4, 4]               0\n",
      "           Conv2d-21           [-1, 1024, 4, 4]         525,312\n",
      "      BatchNorm2d-22           [-1, 1024, 4, 4]           2,048\n",
      "             ReLU-23           [-1, 1024, 4, 4]               0\n",
      "       ConvBlock1-24           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-25           [-1, 1024, 4, 4]       1,049,600\n",
      "      BatchNorm2d-26           [-1, 1024, 4, 4]           2,048\n",
      "             ReLU-27           [-1, 1024, 4, 4]               0\n",
      "       ConvBlock1-28           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-29            [-1, 512, 4, 4]       4,719,104\n",
      "      BatchNorm2d-30            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-31            [-1, 512, 4, 4]               0\n",
      "        ConvBlock-32            [-1, 512, 2, 2]               0\n",
      "================================================================\n",
      "Total params: 7,869,696\n",
      "Trainable params: 7,869,696\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 26.39\n",
      "Params size (MB): 30.02\n",
      "Estimated Total Size (MB): 56.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Classifier().to(device)\n",
    "\n",
    "summary(net, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
